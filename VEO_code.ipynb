{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VEO_code.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lyeslehara1996/PFE/blob/main/VEO_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fM2TXx9wS3pG"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o6vjXy4tVLx"
      },
      "source": [
        "# **Remarque **\n",
        "\n",
        "J'ai importer \"\" from keras.metrics import binary_crossentropy \"\" a la place de Objectives  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEds2E1_ciVQ"
      },
      "source": [
        "from keras.metrics import binary_crossentropy\n",
        "from keras import backend as K\n",
        "from keras.layers import Bidirectional, Dense, Embedding, Input, Lambda, LSTM, RepeatVector, TimeDistributed, Layer, Activation, Dropout\n",
        "from keras.models import Model\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import pandas as pd \n",
        "import re \n",
        "import nltk\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from keras.layers import Dropout\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import string as st\n",
        "SAVEd = False\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from keras.utils.np_utils import to_categorical\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\"\"\" Dataset\"\"\"\n",
        "\n",
        "from keras.wrappers.scikit_learn import KerasClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zychyMJHeGU1"
      },
      "source": [
        "\n",
        "df=pd.read_excel(\"/content/drive/MyDrive/travail/SemEval2017A.xlsx\")\n",
        "\n",
        "df.drop(\"Unnamed: 3\", axis=1, inplace=True)\n",
        "df.drop(\"Unnamed: 4\", axis=1, inplace=True)\n",
        "df.drop(\"Unnamed: 5\", axis=1, inplace=True)\n",
        "df.drop(\"Unnamed: 6\", axis=1, inplace=True)\n",
        "\n",
        "df.head()\n",
        "\n",
        "\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9uCj1GVeGaR"
      },
      "source": [
        "\n",
        "#supprimer les lignes qui contient des valeur null \n",
        "df.Polarity.unique()\n",
        "df.dropna(subset=['Polarity'], inplace=True)\n",
        "df.Polarity.unique()\n",
        "df.info()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "df.Polarity.hist(xlabelsize=14)\n",
        "plt.show()\n",
        "\n",
        "#### transformet les mots en miniscule ######\n",
        "df.Comments=df.Comments.str.lower()\n",
        "df.head() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smikMKeMeGcz"
      },
      "source": [
        "\n",
        "###################STOP WORDS################\n",
        "#STOP WORDS\n",
        "#Tokenization of text\n",
        "\n",
        "tokenizer=ToktokTokenizer()\n",
        "#Setting English stopwords\n",
        "stopword_list=nltk.corpus.stopwords.words('english')\n",
        "\n",
        "#removing the stopwords\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text\n",
        "#Apply function on review column\n",
        "df['Comments']=df['Comments'].apply(remove_stopwords)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxKwaQ-zWkn2"
      },
      "source": [
        "\n",
        "############supprission des caractere spiciaux Dans Commantaire #########\n",
        "df['Comments'] = df['Comments'].apply(lambda x: re.sub(r'https?:\\/\\/\\S+', ' ', str(x)))\n",
        "df['Comments'] = df['Comments'].apply(lambda x: re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", ' ', str(x)))\n",
        "df['Comments'] = df['Comments'].apply(lambda x: re.sub(r'{link}', ' ', str(x)))\n",
        "df['Comments'] = df['Comments'].apply(lambda x: re.sub(r'&[a-z]+;', ' ', str(x)))\n",
        "df['Comments'] = df['Comments'].apply(lambda x: re.sub(r\"[^a-z]\", ' ', str(x)))\n",
        "df['Comments'] = df['Comments'].apply(lambda x: re.sub(r'@mention', ' ', str(x)))\n",
        "df['Comments'] = df['Comments'].apply(lambda x: \" \".join(x.lower() for x in str(x).split()  if len(x)>3 ))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uMLIIh9Wkqk"
      },
      "source": [
        "#######deviser en review and labels ######\n",
        "\n",
        "\n",
        "reviews =  df[['Comments']]\n",
        "labels =  df[['Polarity']]\n",
        "\n",
        "corpus= []\n",
        "for text in reviews['Comments']:\n",
        "    words= [word.lower() for word in word_tokenize(text)]\n",
        "    corpus.append(words)\n",
        "\n",
        "num_words=len(corpus)\n",
        "print(num_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ALKY_-pWktV"
      },
      "source": [
        "####reviews sans ponctuation #######\n",
        "\n",
        "revue_sans_ponctuation=[]\n",
        "for sentence in reviews['Comments']:\n",
        "\n",
        "    revue_sans_ponctuation.append(' '.join(Word.strip(st.punctuation) for Word in sentence.split()))\n",
        "\n",
        "reviews_cleaned = np.asarray(revue_sans_ponctuation)\n",
        "reviews_cleaned\n",
        "\n",
        "\n",
        "\n",
        "review_array = np.asarray(revue_sans_ponctuation)\n",
        "label_array = np.asarray(labels['Polarity'])\n",
        "\n",
        "reviews_labels = np.stack((review_array, label_array), axis = 1)\n",
        "\n",
        "reviews_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rltTo8fGWkwC"
      },
      "source": [
        "########Encoder les polarity  ##############\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(label_array)\n",
        "encoded_labels = encoder.transform(label_array)\n",
        "encoded_labels = to_categorical(encoded_labels)\n",
        "encoded_labels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUv_KyFhWt8_"
      },
      "source": [
        "##### Train and Test\n",
        "review_train, review_test, label_train, label_test = train_test_split(reviews_cleaned, encoded_labels, test_size=0.20, random_state=42)\n",
        "\"\"\"X_train, X_test, y_train, y_test = train_test_split(reviews_cleaned, encoded_labels, test_size=0.20, random_state=42)\"\"\"\n",
        "print(review_train.shape, label_train.shape)\n",
        "print(review_test.shape, label_test.shape)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=2000)\n",
        "tokenizer.fit_on_texts(review_train)\n",
        "\n",
        "review_train = tokenizer.texts_to_sequences(review_train)\n",
        "review_test = tokenizer.texts_to_sequences(review_test)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-17zl4PSWt_2"
      },
      "source": [
        "embeddings_dictionary = dict()\n",
        "glove_file = open('/content/drive/MyDrive/travail/glove.6B.100d.txt', encoding=\"utf8\")\n",
        "\n",
        "for line in glove_file:\n",
        "    records = line.split()\n",
        "    word = records[0]\n",
        "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
        "    embeddings_dictionary [word] = vector_dimensions\n",
        "    \n",
        "glove_file.close()\n",
        "\n",
        "embedding_matrix = zeros((vocab_size, 100))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_dictionary.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[index] = embedding_vector\n",
        "print(label_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9I9LDWmWuCn"
      },
      "source": [
        "\"\"\"\n",
        "remarque 1:\n",
        "\n",
        "Dans train_index et test_index on doit changer (diminuer) la taille de donnee de 1000 et 500 a    500 et 100 par example \n",
        "sinon ce sera pas s'executer  \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "X_train=review_train\n",
        "X_test=review_test\n",
        "y_train=label_train\n",
        "y_test=label_test\n",
        "\n",
        "MAX_LENGTH = 100\n",
        "NUM_WORDS = vocab_size\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=MAX_LENGTH)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=MAX_LENGTH)\n",
        "\n",
        "train_index = np.random.choice(np.arange(X_train.shape[0]), 20, replace=False)   \n",
        "test_index = np.random.choice(np.arange(X_test.shape[0]), 5, replace=False)      \n",
        "\n",
        "X_train = X_train[train_index]\n",
        "y_train = y_train[train_index]\n",
        "\n",
        "X_test = X_test[test_index]\n",
        "y_test = y_test[test_index]\n",
        "\n",
        "temp = np.zeros((X_train.shape[0], MAX_LENGTH, NUM_WORDS))\n",
        "temp[np.expand_dims(np.arange(X_train.shape[0]), axis=0).reshape(X_train.shape[0], 1), np.repeat(np.array([np.arange(MAX_LENGTH)]), X_train.shape[0], axis=0), X_train] = 1\n",
        "\n",
        "Conversion_X_train = temp\n",
        "\n",
        "temp = np.zeros((X_test.shape[0], MAX_LENGTH, NUM_WORDS))\n",
        "temp[np.expand_dims(np.arange(X_test.shape[0]), axis=0).reshape(X_test.shape[0], 1), np.repeat(np.array([np.arange(MAX_LENGTH)]), X_test.shape[0], axis=0), X_test] = 1\n",
        "\n",
        "Conversion_X_test = temp\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "print(Conversion_X_train.shape)\n",
        "print(Conversion_X_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwxfJyZwW_pW"
      },
      "source": [
        "\n",
        "class AttentionCouche(Layer): \n",
        "    def __init__(self):   \n",
        "        self.dim_mots = 20\n",
        "        self.unite_par_seq = 5\n",
        "\n",
        "        super(AttentionCouche, self).__init__()\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "\n",
        "        dim_mots = input_shape[-1] \n",
        "        unite_par_seq = input_shape[-2]\n",
        "        num_units = 1\n",
        "        \n",
        "        self.w = self.add_weight((dim_mots,num_units), initializer='normal')            \n",
        "        self.b = self.add_weight((unite_par_seq,num_units), initializer='zero')\n",
        "       \n",
        "        \n",
        "        super(AttentionCouche, self).build(input_shape)\n",
        "        \n",
        "    def call(self, x):\n",
        "        e = K.squeeze(K.tanh(K.dot(x,self.w)+self.b), axis=1) \n",
        "        a = K.softmax(e, axis=1)\n",
        "        a = K.expand_dims(a,axis=1)\n",
        "        output = x*a\n",
        "        output = K.sum(output, axis=1)\n",
        "        \n",
        "        return a,output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnptxTPwTlHU"
      },
      "source": [
        "**remarque trouvé**\n",
        "\n",
        "j'ai diminuer la taille des donne jusqu'a dim_mots = 20 et unite_par_seq = 5\n",
        "\n",
        "j'ai essayer de pas executer l'attention mais donne une erreur dans la fonction fit . \n",
        "\n",
        "et lorsque j'execute l'attention il me donne une erreur dans leur implementation **texte en gras**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmkwJK0RW_sw"
      },
      "source": [
        "batch_size = 128\n",
        "vocab_size=vocab_size\n",
        "max_length=100 \n",
        "dim_latent=200\n",
        "intermediate_dim=500\n",
        "\n",
        "encoder = None\n",
        "decoder = None\n",
        "predicter_opinion = None\n",
        "autoencoder = None\n",
        "\n",
        "x = Input(shape=(max_length,))\n",
        "x_embed = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length)(x)\n",
        "\n",
        "\n",
        "def Encodeur(x, dim_latent=200, max_length=100, epsilon_std=0.01):\n",
        "    lstm1 = Bidirectional(LSTM(500, return_sequences=True), merge_mode='concat', name ='LSTM1')(x)\n",
        "    lstm2 = Bidirectional(LSTM(250, return_sequences=False), merge_mode='concat', name ='LSTM2')(lstm1)\n",
        "    # attention = AttentionCouche()(lstm2)\n",
        "    h = Dense(320, activation='relu', name='dense_1')(lstm2)\n",
        "    \n",
        "    def sampling(args):\n",
        "        z_mean, z_log_var = args\n",
        "        batch = K.shape(z_mean)[0]\n",
        "        # by default, random_normal has mean=0 and std=1.0\n",
        "        epsilon = K.random_normal(shape=(batch, dim_latent), mean=0., stddev=epsilon_std)\n",
        "        return z_mean + K.exp(z_log_var / 2) * epsilon\n",
        "    \n",
        "    \n",
        "    z_mean = Dense(dim_latent, name='z_mean', activation='linear')(h)\n",
        "    z_log_var = Dense(dim_latent, name='z_log_var', activation='linear')(h)\n",
        "    z = Lambda(sampling, output_shape=(dim_latent,), name='z')([z_mean, z_log_var])\n",
        "\n",
        "\n",
        "    def fct_loss(x, x_decoded_mean):\n",
        "        x = K.flatten(x)\n",
        "        x_decoded_mean = K.flatten(x_decoded_mean)\n",
        "        xent_loss = max_length * binary_crossentropy(x, x_decoded_mean)\n",
        "        kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "        return xent_loss + kl_loss\n",
        "    \n",
        "    return (fct_loss, z)\n",
        "    \n",
        "     \n",
        "fct_loss,encoded = Encodeur(x_embed, dim_latent= dim_latent, max_length= max_length)\n",
        "encoder = tf.keras.models.Model(inputs=x, outputs=encoded)\n",
        "encoder.summary()\n",
        "\n",
        "\n",
        "def Codage_opinion(encoded):\n",
        "    h = Dense(100, activation='linear')(encoded)     \n",
        "    return Dense(3, activation='softmax', name='pred')(h)\n",
        "\n",
        "\n",
        "encoded_input = Input(shape=(dim_latent,))\n",
        "opinion_lab = Codage_opinion(encoded_input)\n",
        "predicter_opinion = Model(encoded_input, opinion_lab)\n",
        "predicter_opinion.summary()\n",
        "\n",
        "\n",
        "def Decodeur(encoded, vocab_size, max_length):\n",
        "    repeated_context = RepeatVector(max_length)(encoded)\n",
        "    h = LSTM(500, return_sequences=True, name='dec_lstm_1')(repeated_context)\n",
        "    h = LSTM(250, return_sequences=True, name='dec_lstm_2')(h)\n",
        "    decoded = TimeDistributed(Dense(vocab_size, activation='softmax'), name='decoded_mean')(h)        \n",
        "    return decoded\n",
        "\n",
        "\n",
        "decoded = Decodeur(encoded_input, vocab_size, max_length)\n",
        "decoder = Model(encoded_input, decoded)\n",
        "decoder.summary()\n",
        "\n",
        "autoencoder = Model(inputs=x, outputs=[Decodeur(encoded, vocab_size, max_length), Codage_opinion(encoded)])\n",
        "tf.config.experimental_run_functions_eagerly(True)\n",
        "autoencoder.compile(optimizer='Adam', loss=[fct_loss, 'binary_crossentropy'], metrics=['accuracy'])\n",
        "\n",
        "autoencoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGSS9Bgn6CJI"
      },
      "source": [
        "        \n",
        "autoencoder.fit(x=X_train, y={'decoded_mean': Conversion_X_train, 'pred': y_train}, batch_size=128, epochs=10, validation_data=(X_test, {'decoded_mean': Conversion_X_test, 'pred':  y_test}))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}